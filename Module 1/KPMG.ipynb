{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as DT\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transactions\n",
    "\n",
    "Upon loading the first sheet of our dataset (Transactions) and reading its head, we see that the first row is just a notice. Hence, we skip it while reading. Then, we check its shape to see the number of rows (transaction entries) and columns (features). \n",
    "\n",
    "Next, we check the number of null values in all columns. We observe that the column online_order has 360 null values. In order to ensure data completness and quality, we can drop the rows with these null values. Depending on how we want to use our data later, we can also filter out rows on the basis of whether the order was online or not.\n",
    "\n",
    "Similarly, we drop all rows that have null values for the brand name. After this, when we check null values again, we see that all rows with any null values have been dropped.\n",
    "\n",
    "Finally, we change the product_first_sold_date from a vague string to a date format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Customer Id Entries:  19999\n",
      "Number of Features:  13 \n",
      "\n",
      "Number of Initial Null Values: \n",
      "transaction_id               0\n",
      "product_id                   0\n",
      "customer_id                  0\n",
      "transaction_date             0\n",
      "online_order               360\n",
      "order_status                 0\n",
      "brand                      197\n",
      "product_line               197\n",
      "product_class              197\n",
      "product_size               197\n",
      "list_price                   0\n",
      "standard_cost              197\n",
      "product_first_sold_date    197\n",
      "dtype: int64 \n",
      "\n",
      "   transaction_id  product_id  customer_id transaction_date  online_order  \\\n",
      "0               1           2         2950       2017-02-25           0.0   \n",
      "1               2           3         3120       2017-05-21           1.0   \n",
      "2               3          37          402       2017-10-16           0.0   \n",
      "\n",
      "  order_status          brand product_line product_class product_size  \\\n",
      "0     Approved          Solex     Standard        medium       medium   \n",
      "1     Approved  Trek Bicycles     Standard        medium        large   \n",
      "2     Approved     OHM Cycles     Standard           low       medium   \n",
      "\n",
      "   list_price  standard_cost product_first_sold_date  \n",
      "0       71.49          53.62              2012-12-04  \n",
      "1     2091.47         388.92              2014-03-05  \n",
      "2     1793.43         248.82              1999-07-22  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", sheet_name=1, skiprows=1)\n",
    "\n",
    "rows = df.shape[0]\n",
    "cols = df.shape[1]\n",
    "print(\"Number of Customer Id Entries: \", rows-1)\n",
    "print(\"Number of Features: \", cols, \"\\n\" )\n",
    "\n",
    "# counting null values\n",
    "print(\"Number of Initial Null Values: \")\n",
    "print(df.isnull().sum(axis = 0),\"\\n\")\n",
    "# print(df.head(3))\n",
    "\n",
    "# filtering out null values & offline orders\n",
    "df = df[df['online_order'].notnull()]\n",
    "df = df[df['brand'].notnull()]\n",
    "# print(df.isnull().sum(axis = 0))\n",
    "\n",
    "# converting string to date format\n",
    "df['product_first_sold_date'] = pd.to_datetime(df['product_first_sold_date'],unit='d',origin='1900-01-01')\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking for Duplicates\n",
    "\n",
    "In order to ensure uniquness of data, we must ensure that there are no unncessary duplicates. In this case, transaction ids should not be repeated over rows. Hence, we check for duplicate transaction ids and if any are found, we drop rows as required. Luckily, in this case, there are no duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(df['transaction_id'])-len(df['transaction_id'].drop_duplicates()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Profits\n",
    "\n",
    "Since our ultimate goal is to help Sprocket Central Pty Ltd grow its business, we must consider their profits. For this, we can add a new column for net profits. This can help us achieve relevancy (data items with value meta data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      17.87\n",
      "1    1702.55\n",
      "2    1544.61\n",
      "Name: net_profit, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df['net_profit'] = df['list_price'] - df['standard_cost']\n",
    "print(df.net_profit.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataset appears to be in a good shape, let's save our dataset into a dictionary that we will use later to generate an Excel file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter('KPMG_TASK1.xlsx', engine='xlsxwriter')\n",
    "dataFrames = {'Transactions': df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Demographics\n",
    "\n",
    "Similar to the Transactions dataset, upon loading the third sheet (Customer Demographics), we see that the first row is just a notice. Hence, we skip it while reading it. The default columns seems quite absurd and useless. It's best to drop it in the early stage.  \n",
    "\n",
    "Then, we check its shape to see the number of rows and columns (features). Next, we check the number of null values in all columns. We observe that there are around 125 N/A values for the last name. However, since last names might not have any impact on our business strategy, we can choose to ignore that column. However, DOB is a valuable asset for us. Hence, we drop the rows which have null values for the DOB.\n",
    "\n",
    "Similarly, job titles could also be useful for us. Hence, we drop the rows which have null values for the job title as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Customer Id Entries:  3999\n",
      "Number of Features:  12 \n",
      "\n",
      "Number of Initial Null Values: \n",
      "customer_id                              0\n",
      "first_name                               0\n",
      "last_name                              125\n",
      "gender                                   0\n",
      "past_3_years_bike_related_purchases      0\n",
      "DOB                                     87\n",
      "job_title                              506\n",
      "job_industry_category                  656\n",
      "wealth_segment                           0\n",
      "deceased_indicator                       0\n",
      "owns_car                                 0\n",
      "tenure                                  87\n",
      "dtype: int64 \n",
      "\n",
      "   customer_id      first_name  last_name  gender  \\\n",
      "0            1         Laraine  Medendorp       F   \n",
      "1            2             Eli    Bockman    Male   \n",
      "2            3           Arlin     Dearle    Male   \n",
      "4            5  Sheila-kathryn     Calton  Female   \n",
      "7            8             Rod      Inder    Male   \n",
      "\n",
      "   past_3_years_bike_related_purchases        DOB               job_title  \\\n",
      "0                                   93 1953-10-12     Executive Secretary   \n",
      "1                                   81 1980-12-16  Administrative Officer   \n",
      "2                                   61 1954-01-20      Recruiting Manager   \n",
      "4                                   56 1977-05-13           Senior Editor   \n",
      "7                                   31 1962-03-30         Media Manager I   \n",
      "\n",
      "  job_industry_category     wealth_segment deceased_indicator owns_car  tenure  \n",
      "0                Health      Mass Customer                  N      Yes    11.0  \n",
      "1    Financial Services      Mass Customer                  N      Yes    16.0  \n",
      "2              Property      Mass Customer                  N      Yes    15.0  \n",
      "4                   NaN  Affluent Customer                  N      Yes     8.0  \n",
      "7                   NaN      Mass Customer                  N       No     7.0  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", sheet_name=3, skiprows=1)\n",
    "df = df.drop(['default'], axis=1)\n",
    "\n",
    "rows = df.shape[0]\n",
    "cols = df.shape[1]\n",
    "print(\"Number of Customer Id Entries: \", rows-1)\n",
    "print(\"Number of Features: \", cols, \"\\n\" )\n",
    "\n",
    "print(\"Number of Initial Null Values: \")\n",
    "print(df.isnull().sum(axis = 0),\"\\n\")\n",
    "df = df[df['DOB'].notnull()]\n",
    "df = df[df['job_title'].notnull()]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contradiction in Genders\n",
    "\n",
    "We notice that the values in the gender column are not consistent. By looking at the value counts, we observe that F and Female both represent the same thing. Similarly, M and Male represent the both thing. Additionally, terms U and Femal are also present in the dataset. \n",
    "\n",
    "To fix this, we can replace Female/Femal with F and Male with M everywhere. Then, we can get rid of the row where gender is undefined (U)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Female    1769\n",
      "Male      1643\n",
      "U            1\n",
      "F            1\n",
      "Femal        1\n",
      "M            1\n",
      "Name: gender, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.gender.value_counts(),\"\\n\")\n",
    "df[\"gender\"].replace({\"Female\": \"F\", \"Male\": \"M\", \"Femal\": \"F\"}, inplace=True)\n",
    "df = df[df['gender'] != \"U\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Ages\n",
    "\n",
    "When preparing business and marketing strategies, targeting the right age bracket is quite essential. Hence, we need to know the exact ages of our customers. For this, we can add a new column for customer ages. We can calculate ages by subtracting the DOB from the current date. After this, the dataset seems to look good to go. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customer_id first_name  last_name gender  \\\n",
      "0            1    Laraine  Medendorp      F   \n",
      "1            2        Eli    Bockman      M   \n",
      "2            3      Arlin     Dearle      M   \n",
      "\n",
      "   past_3_years_bike_related_purchases        DOB               job_title  \\\n",
      "0                                   93 1953-10-12     Executive Secretary   \n",
      "1                                   81 1980-12-16  Administrative Officer   \n",
      "2                                   61 1954-01-20      Recruiting Manager   \n",
      "\n",
      "  job_industry_category wealth_segment deceased_indicator owns_car  tenure  \\\n",
      "0                Health  Mass Customer                  N      Yes    11.0   \n",
      "1    Financial Services  Mass Customer                  N      Yes    16.0   \n",
      "2              Property  Mass Customer                  N      Yes    15.0   \n",
      "\n",
      "   age  \n",
      "0   66  \n",
      "1   39  \n",
      "2   66  \n"
     ]
    }
   ],
   "source": [
    "now = pd.Timestamp('now')\n",
    "df['age'] = (now - df['DOB']).astype('<m8[Y]')\n",
    "df['age'] = df['age'].astype(np.int64)\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrames['CustomerDemographic'] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Addresses \n",
    "\n",
    "Just like the previous two datasets, this one also has a notice in the first row. Hence, we skip it while loading our dataset. Then, we check its shape of our dataset to see the number of rows and columns (features). \n",
    "\n",
    "Next, we check the number of null values in all columns. Luckily, there are none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Customer Id Entries:  3998\n",
      "Number of Features:  6 \n",
      "\n",
      "Number of Initial Null Values: \n",
      "customer_id           0\n",
      "address               0\n",
      "postcode              0\n",
      "state                 0\n",
      "country               0\n",
      "property_valuation    0\n",
      "dtype: int64 \n",
      "\n",
      "   customer_id              address  postcode            state    country  \\\n",
      "0            1   060 Morning Avenue      2016  New South Wales  Australia   \n",
      "1            2  6 Meadow Vale Court      2153  New South Wales  Australia   \n",
      "2            4   0 Holy Cross Court      4211              QLD  Australia   \n",
      "3            5  17979 Del Mar Point      2448  New South Wales  Australia   \n",
      "4            6     9 Oakridge Court      3216              VIC  Australia   \n",
      "\n",
      "   property_valuation  \n",
      "0                  10  \n",
      "1                  10  \n",
      "2                   9  \n",
      "3                   4  \n",
      "4                   9  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(\"KPMG_VI_New_raw_data_update_final.xlsx\", sheet_name=4, skiprows=1)\n",
    "\n",
    "rows = df.shape[0]\n",
    "cols = df.shape[1]\n",
    "print(\"Number of Customer Id Entries: \", rows-1)\n",
    "print(\"Number of Features: \", cols, \"\\n\" )\n",
    "\n",
    "print(\"Number of Initial Null Values: \")\n",
    "print(df.isnull().sum(axis = 0),\"\\n\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contradiction in States\n",
    "\n",
    "Upon seeing the head of our dataset, we observe something strange. Some state names are written full while others are abbreviations. To investigate further, we can check the value counts. After checking the value counts, we realize that there's a contradition in state names. For instance, New South Wales and NSW is the same but are written separately. Same is the case with Victoria and VIC. Hence, we make the necessary replacements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NSW                2054\n",
      "VIC                 939\n",
      "QLD                 838\n",
      "New South Wales      86\n",
      "Victoria             82\n",
      "Name: state, dtype: int64 \n",
      "\n",
      "   customer_id              address  postcode state    country  \\\n",
      "0            1   060 Morning Avenue      2016   NSW  Australia   \n",
      "1            2  6 Meadow Vale Court      2153   NSW  Australia   \n",
      "2            4   0 Holy Cross Court      4211   QLD  Australia   \n",
      "3            5  17979 Del Mar Point      2448   NSW  Australia   \n",
      "4            6     9 Oakridge Court      3216   VIC  Australia   \n",
      "\n",
      "   property_valuation  \n",
      "0                  10  \n",
      "1                  10  \n",
      "2                   9  \n",
      "3                   4  \n",
      "4                   9  \n"
     ]
    }
   ],
   "source": [
    "print(df.state.value_counts(),\"\\n\")\n",
    "df[\"state\"].replace({\"New South Wales\": \"NSW\", \"Victoria\": \"VIC\"}, inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything appears to be better now. As the final step, let's merge the datasets into a single Excel sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrames['CustomerAddress'] = df\n",
    "for sheet, frame in dataFrames.items():\n",
    "    frame.to_excel(writer, sheet_name = sheet, index=False)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
